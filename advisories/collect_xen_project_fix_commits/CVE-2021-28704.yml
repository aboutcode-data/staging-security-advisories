advisory_id: CVE-2021-28704
datasource_id: collect_xen_project_fix_commits/CVE-2021-28704
datasource_url: https://github.com/xen-project/xen
aliases: []
summary: |
  398f7aec5dab6ad0853a8b069b48ae304e059fa4:x86/PoD: deal with misaligned GFNs

  Users of XENMEM_decrease_reservation and XENMEM_populate_physmap aren't
  required to pass in order-aligned GFN values. (While I consider this
  bogus, I don't think we can fix this there, as that might break existing
  code, e.g Linux'es swiotlb, which - while affecting PV only - until
  recently had been enforcing only page alignment on the original
  allocation.) Only non-PoD code paths (guest_physmap_{add,remove}_page(),
  p2m_set_entry()) look to be dealing with this properly (in part by being
  implemented inefficiently, handling every 4k page separately).

  Introduce wrappers taking care of splitting the incoming request into
  aligned chunks, without putting much effort in trying to determine the
  largest possible chunk at every iteration.

  Also "handle" p2m_set_entry() failure for non-order-0 requests by
  crashing the domain in one more place. Alongside putting a log message
  there, also add one to the other similar path.

  Note regarding locking: This is left in the actual worker functions on
  the assumption that callers aren't guaranteed atomicity wrt acting on
  multiple pages at a time. For mis-aligned GFNs gfn_lock() wouldn't have
  locked the correct GFN range anyway, if it didn't simply resolve to
  p2m_lock(), and for well-behaved callers there continues to be only a
  single iteration, i.e. behavior is unchanged for them. (FTAOD pulling
  out just pod_lock() into p2m_pod_decrease_reservation() would result in
  a lock order violation.)

  This is CVE-2021-28704 and CVE-2021-28707 / part of XSA-388.

  Fixes: 3c352011c0d3 ("x86/PoD: shorten certain operations on higher order ranges")
  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
  master commit: 182c737b9ba540ebceb1433f3940fbed6eac4ea9
  master date: 2021-11-22 12:27:30 +0000
  d94d006ed36084914c2931641b724ae262e3fb80:x86/PoD: deal with misaligned GFNs

  Users of XENMEM_decrease_reservation and XENMEM_populate_physmap aren't
  required to pass in order-aligned GFN values. (While I consider this
  bogus, I don't think we can fix this there, as that might break existing
  code, e.g Linux'es swiotlb, which - while affecting PV only - until
  recently had been enforcing only page alignment on the original
  allocation.) Only non-PoD code paths (guest_physmap_{add,remove}_page(),
  p2m_set_entry()) look to be dealing with this properly (in part by being
  implemented inefficiently, handling every 4k page separately).

  Introduce wrappers taking care of splitting the incoming request into
  aligned chunks, without putting much effort in trying to determine the
  largest possible chunk at every iteration.

  Also "handle" p2m_set_entry() failure for non-order-0 requests by
  crashing the domain in one more place. Alongside putting a log message
  there, also add one to the other similar path.

  Note regarding locking: This is left in the actual worker functions on
  the assumption that callers aren't guaranteed atomicity wrt acting on
  multiple pages at a time. For mis-aligned GFNs gfn_lock() wouldn't have
  locked the correct GFN range anyway, if it didn't simply resolve to
  p2m_lock(), and for well-behaved callers there continues to be only a
  single iteration, i.e. behavior is unchanged for them. (FTAOD pulling
  out just pod_lock() into p2m_pod_decrease_reservation() would result in
  a lock order violation.)

  This is CVE-2021-28704 and CVE-2021-28707 / part of XSA-388.

  Fixes: 3c352011c0d3 ("x86/PoD: shorten certain operations on higher order ranges")
  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
  master commit: 182c737b9ba540ebceb1433f3940fbed6eac4ea9
  master date: 2021-11-22 12:27:30 +0000
  7f654ea88ee6100f5948f383a38254be8c28a255:x86/PoD: deal with misaligned GFNs

  Users of XENMEM_decrease_reservation and XENMEM_populate_physmap aren't
  required to pass in order-aligned GFN values. (While I consider this
  bogus, I don't think we can fix this there, as that might break existing
  code, e.g Linux'es swiotlb, which - while affecting PV only - until
  recently had been enforcing only page alignment on the original
  allocation.) Only non-PoD code paths (guest_physmap_{add,remove}_page(),
  p2m_set_entry()) look to be dealing with this properly (in part by being
  implemented inefficiently, handling every 4k page separately).

  Introduce wrappers taking care of splitting the incoming request into
  aligned chunks, without putting much effort in trying to determine the
  largest possible chunk at every iteration.

  Also "handle" p2m_set_entry() failure for non-order-0 requests by
  crashing the domain in one more place. Alongside putting a log message
  there, also add one to the other similar path.

  Note regarding locking: This is left in the actual worker functions on
  the assumption that callers aren't guaranteed atomicity wrt acting on
  multiple pages at a time. For mis-aligned GFNs gfn_lock() wouldn't have
  locked the correct GFN range anyway, if it didn't simply resolve to
  p2m_lock(), and for well-behaved callers there continues to be only a
  single iteration, i.e. behavior is unchanged for them. (FTAOD pulling
  out just pod_lock() into p2m_pod_decrease_reservation() would result in
  a lock order violation.)

  This is CVE-2021-28704 and CVE-2021-28707 / part of XSA-388.

  Fixes: 3c352011c0d3 ("x86/PoD: shorten certain operations on higher order ranges")
  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
  master commit: 182c737b9ba540ebceb1433f3940fbed6eac4ea9
  master date: 2021-11-22 12:27:30 +0000
  1da54becd5de7e55b2b66665bedc2f30964dbd33:x86/PoD: deal with misaligned GFNs

  Users of XENMEM_decrease_reservation and XENMEM_populate_physmap aren't
  required to pass in order-aligned GFN values. (While I consider this
  bogus, I don't think we can fix this there, as that might break existing
  code, e.g Linux'es swiotlb, which - while affecting PV only - until
  recently had been enforcing only page alignment on the original
  allocation.) Only non-PoD code paths (guest_physmap_{add,remove}_page(),
  p2m_set_entry()) look to be dealing with this properly (in part by being
  implemented inefficiently, handling every 4k page separately).

  Introduce wrappers taking care of splitting the incoming request into
  aligned chunks, without putting much effort in trying to determine the
  largest possible chunk at every iteration.

  Also "handle" p2m_set_entry() failure for non-order-0 requests by
  crashing the domain in one more place. Alongside putting a log message
  there, also add one to the other similar path.

  Note regarding locking: This is left in the actual worker functions on
  the assumption that callers aren't guaranteed atomicity wrt acting on
  multiple pages at a time. For mis-aligned GFNs gfn_lock() wouldn't have
  locked the correct GFN range anyway, if it didn't simply resolve to
  p2m_lock(), and for well-behaved callers there continues to be only a
  single iteration, i.e. behavior is unchanged for them. (FTAOD pulling
  out just pod_lock() into p2m_pod_decrease_reservation() would result in
  a lock order violation.)

  This is CVE-2021-28704 and CVE-2021-28707 / part of XSA-388.

  Fixes: 3c352011c0d3 ("x86/PoD: shorten certain operations on higher order ranges")
  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
  master commit: 182c737b9ba540ebceb1433f3940fbed6eac4ea9
  master date: 2021-11-22 12:27:30 +0000
  182c737b9ba540ebceb1433f3940fbed6eac4ea9:x86/PoD: deal with misaligned GFNs

  Users of XENMEM_decrease_reservation and XENMEM_populate_physmap aren't
  required to pass in order-aligned GFN values. (While I consider this
  bogus, I don't think we can fix this there, as that might break existing
  code, e.g Linux'es swiotlb, which - while affecting PV only - until
  recently had been enforcing only page alignment on the original
  allocation.) Only non-PoD code paths (guest_physmap_{add,remove}_page(),
  p2m_set_entry()) look to be dealing with this properly (in part by being
  implemented inefficiently, handling every 4k page separately).

  Introduce wrappers taking care of splitting the incoming request into
  aligned chunks, without putting much effort in trying to determine the
  largest possible chunk at every iteration.

  Also "handle" p2m_set_entry() failure for non-order-0 requests by
  crashing the domain in one more place. Alongside putting a log message
  there, also add one to the other similar path.

  Note regarding locking: This is left in the actual worker functions on
  the assumption that callers aren't guaranteed atomicity wrt acting on
  multiple pages at a time. For mis-aligned GFNs gfn_lock() wouldn't have
  locked the correct GFN range anyway, if it didn't simply resolve to
  p2m_lock(), and for well-behaved callers there continues to be only a
  single iteration, i.e. behavior is unchanged for them. (FTAOD pulling
  out just pod_lock() into p2m_pod_decrease_reservation() would result in
  a lock order violation.)

  This is CVE-2021-28704 and CVE-2021-28707 / part of XSA-388.

  Fixes: 3c352011c0d3 ("x86/PoD: shorten certain operations on higher order ranges")
  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Roger Pau Monné <roger.pau@citrix.com>
impacted_packages:
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 7f654ea88ee6100f5948f383a38254be8c28a255
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 398f7aec5dab6ad0853a8b069b48ae304e059fa4
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 1da54becd5de7e55b2b66665bedc2f30964dbd33
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: d94d006ed36084914c2931641b724ae262e3fb80
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 182c737b9ba540ebceb1433f3940fbed6eac4ea9
    introduced_in_commits: []
severities: []
weaknesses: []
references:
  - url: https://github.com/xen-project/xen/tree/182c737b9ba540ebceb1433f3940fbed6eac4ea9
    reference_type: commit
    reference_id: 182c737b9ba540ebceb1433f3940fbed6eac4ea9
  - url: https://github.com/xen-project/xen/tree/1da54becd5de7e55b2b66665bedc2f30964dbd33
    reference_type: commit
    reference_id: 1da54becd5de7e55b2b66665bedc2f30964dbd33
  - url: https://github.com/xen-project/xen/tree/398f7aec5dab6ad0853a8b069b48ae304e059fa4
    reference_type: commit
    reference_id: 398f7aec5dab6ad0853a8b069b48ae304e059fa4
  - url: https://github.com/xen-project/xen/tree/7f654ea88ee6100f5948f383a38254be8c28a255
    reference_type: commit
    reference_id: 7f654ea88ee6100f5948f383a38254be8c28a255
  - url: https://github.com/xen-project/xen/tree/d94d006ed36084914c2931641b724ae262e3fb80
    reference_type: commit
    reference_id: d94d006ed36084914c2931641b724ae262e3fb80
