advisory_id: CVE-2018-3646
datasource_id: collect_xen_project_fix_commits/CVE-2018-3646
datasource_url: https://github.com/xen-project/xen
aliases: []
summary: |
  ef1b64877424016c90400963adff056e9199e667:amend "x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH"

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Acked-by: Andrew Cooper <andrew.cooper3@citrix.com>
  e1b03b03b199bd206c81286b4f51b6a681123eda:xl.conf: Add global affinity masks

  XSA-273 involves one hyperthread being able to use Spectre-like
  techniques to "spy" on another thread.  The details are somewhat
  complicated, but the upshot is that after all Xen-based mitigations
  have been applied:

  * PV guests cannot spy on sibling threads
  * HVM guests can spy on sibling threads

  (NB that for purposes of this vulnerability, PVH and HVM guests are
  identical.  Whenever this comment refers to 'HVM', this includes PVH.)

  There are many possible mitigations to this, including disabling
  hyperthreading entirely.  But another solution would be:

  * Specify some cores as PV-only, others as PV or HVM
  * Allow HVM guests to only run on thread 0 of the "HVM-or-PV" cores
  * Allow PV guests to run on the above cores, as well as any thread of the PV-only cores.

  For example, suppose you had 16 threads across 8 cores (0-7).  You
  could specify 0-3 as PV-only, and 4-7 as HVM-or-PV.  Then you'd set
  the affinity of the HVM guests as follows (binary representation):

  0000000010101010

  And the affinity of the PV guests as follows:

  1111111110101010

  In order to make this easy, this patches introduces three "global affinity
  masks", placed in xl.conf:

      vm.cpumask
      vm.hvm.cpumask
      vm.pv.cpumask

  These are parsed just like the 'cpus' and 'cpus_soft' options in the
  per-domain xl configuration files.  The resulting mask is AND-ed with
  whatever mask results at the end of the xl configuration file.
  `vm.cpumask` would be applied to all guest types, `vm.hvm.cpumask`
  would be applied to HVM and PVH guest types, and `vm.pv.cpumask`
  would be applied to PV guest types.

  The idea would be that to implement the above mask across all your
  VMs, you'd simply add the following two lines to the configuration
  file:

      vm.hvm.cpumask=8,10,12,14
      vm.pv.cpumask=0-8,10,12,14

  See xl.conf manpage for details.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: George Dunlap <george.dunlap@citrix.com>
  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  (cherry picked from commit aa67b97ed34279c43a43d9ca46727b5746caa92e)

  PVH guest type in toolstack is not available in this version of Xen.
  Change code and manpage to cope. Also xl is still part of libxl in
  thsi version, manually backport code to relevant places.

  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  c932ff17fc61a6d4c8041e077592800eec54e9f1:x86/msr: Virtualise MSR_FLUSH_CMD for guests

  Guests (outside of the nested virt case, which isn't supported yet) don't need
  L1D_FLUSH for their L1TF mitigations, but offering/emulating MSR_FLUSH_CMD is
  easy and doesn't pose an issue for Xen.

  The MSR is offered to HVM guests only.  PV guests attempting to use it would
  trap for emulation, and the L1D cache would fill long before the return to
  guest context.  As such, PV guests can't make any use of the L1D_FLUSH
  functionality.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fd9823faf9df057a69a9a53c2e100691d3f4267c)
  b2449f57ae15e7899f37e58d6d4abf07ef29f95b:x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit 3563fc2b2731a63fd7e8372ab0f5cef205bf8477)
  9b8375a272ad02d8d0c229b3e3e7989e852734d8:xl.conf: Add global affinity masks

  XSA-273 involves one hyperthread being able to use Spectre-like
  techniques to "spy" on another thread.  The details are somewhat
  complicated, but the upshot is that after all Xen-based mitigations
  have been applied:

  * PV guests cannot spy on sibling threads
  * HVM guests can spy on sibling threads

  (NB that for purposes of this vulnerability, PVH and HVM guests are
  identical.  Whenever this comment refers to 'HVM', this includes PVH.)

  There are many possible mitigations to this, including disabling
  hyperthreading entirely.  But another solution would be:

  * Specify some cores as PV-only, others as PV or HVM
  * Allow HVM guests to only run on thread 0 of the "HVM-or-PV" cores
  * Allow PV guests to run on the above cores, as well as any thread of the PV-only cores.

  For example, suppose you had 16 threads across 8 cores (0-7).  You
  could specify 0-3 as PV-only, and 4-7 as HVM-or-PV.  Then you'd set
  the affinity of the HVM guests as follows (binary representation):

  0000000010101010

  And the affinity of the PV guests as follows:

  1111111110101010

  In order to make this easy, this patches introduces three "global affinity
  masks", placed in xl.conf:

      vm.cpumask
      vm.hvm.cpumask
      vm.pv.cpumask

  These are parsed just like the 'cpus' and 'cpus_soft' options in the
  per-domain xl configuration files.  The resulting mask is AND-ed with
  whatever mask results at the end of the xl configuration file.
  `vm.cpumask` would be applied to all guest types, `vm.hvm.cpumask`
  would be applied to HVM and PVH guest types, and `vm.pv.cpumask`
  would be applied to PV guest types.

  The idea would be that to implement the above mask across all your
  VMs, you'd simply add the following two lines to the configuration
  file:

      vm.hvm.cpumask=8,10,12,14
      vm.pv.cpumask=0-8,10,12,14

  See xl.conf manpage for details.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: George Dunlap <george.dunlap@citrix.com>
  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  (cherry picked from commit aa67b97ed34279c43a43d9ca46727b5746caa92e)

  PVH guest type in toolstack is not available in this version of Xen.
  Change code and manpage to cope. Also xl is still part of libxl in
  thsi version, manually backport code to relevant places.

  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  f05a33e327b87b30a14b6926e232c712ee7c4a1d:x86/msr: Virtualise MSR_FLUSH_CMD for guests

  Guests (outside of the nested virt case, which isn't supported yet) don't need
  L1D_FLUSH for their L1TF mitigations, but offering/emulating MSR_FLUSH_CMD is
  easy and doesn't pose an issue for Xen.

  The MSR is offered to HVM guests only.  PV guests attempting to use it would
  trap for emulation, and the L1D cache would fill long before the return to
  guest context.  As such, PV guests can't make any use of the L1D_FLUSH
  functionality.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fd9823faf9df057a69a9a53c2e100691d3f4267c)
  f440b31efe42e988660812d33c2625bb061ae9ca:x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit 3563fc2b2731a63fd7e8372ab0f5cef205bf8477)
  d95b5bb31e6d4361e356f0ff0853b6bb172a8b6a:xl.conf: Add global affinity masks

  XSA-273 involves one hyperthread being able to use Spectre-like
  techniques to "spy" on another thread.  The details are somewhat
  complicated, but the upshot is that after all Xen-based mitigations
  have been applied:

  * PV guests cannot spy on sibling threads
  * HVM guests can spy on sibling threads

  (NB that for purposes of this vulnerability, PVH and HVM guests are
  identical.  Whenever this comment refers to 'HVM', this includes PVH.)

  There are many possible mitigations to this, including disabling
  hyperthreading entirely.  But another solution would be:

  * Specify some cores as PV-only, others as PV or HVM
  * Allow HVM guests to only run on thread 0 of the "HVM-or-PV" cores
  * Allow PV guests to run on the above cores, as well as any thread of the PV-only cores.

  For example, suppose you had 16 threads across 8 cores (0-7).  You
  could specify 0-3 as PV-only, and 4-7 as HVM-or-PV.  Then you'd set
  the affinity of the HVM guests as follows (binary representation):

  0000000010101010

  And the affinity of the PV guests as follows:

  1111111110101010

  In order to make this easy, this patches introduces three "global affinity
  masks", placed in xl.conf:

      vm.cpumask
      vm.hvm.cpumask
      vm.pv.cpumask

  These are parsed just like the 'cpus' and 'cpus_soft' options in the
  per-domain xl configuration files.  The resulting mask is AND-ed with
  whatever mask results at the end of the xl configuration file.
  `vm.cpumask` would be applied to all guest types, `vm.hvm.cpumask`
  would be applied to HVM and PVH guest types, and `vm.pv.cpumask`
  would be applied to PV guest types.

  The idea would be that to implement the above mask across all your
  VMs, you'd simply add the following two lines to the configuration
  file:

      vm.hvm.cpumask=8,10,12,14
      vm.pv.cpumask=0-8,10,12,14

  See xl.conf manpage for details.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: George Dunlap <george.dunlap@citrix.com>
  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  (cherry picked from commit aa67b97ed34279c43a43d9ca46727b5746caa92e)

  PVH guest type in toolstack is not available in this version of Xen.
  Change code and manpage to cope. Also xl is still part of libxl in
  thsi version, manually backport code to relevant places.

  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  1f56fba4865d9acbd075df88ec67ee6358f11b63:x86/msr: Virtualise MSR_FLUSH_CMD for guests

  Guests (outside of the nested virt case, which isn't supported yet) don't need
  L1D_FLUSH for their L1TF mitigations, but offering/emulating MSR_FLUSH_CMD is
  easy and doesn't pose an issue for Xen.

  The MSR is offered to HVM guests only.  PV guests attempting to use it would
  trap for emulation, and the L1D cache would fill long before the return to
  guest context.  As such, PV guests can't make any use of the L1D_FLUSH
  functionality.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fd9823faf9df057a69a9a53c2e100691d3f4267c)
  5464d5f0c9cf030186ba88c246a3eb96f67277fc:x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit 3563fc2b2731a63fd7e8372ab0f5cef205bf8477)
  14f90aaef8d441cbdece5b74829e85e767fb196c:xl.conf: Add global affinity masks

  XSA-273 involves one hyperthread being able to use Spectre-like
  techniques to "spy" on another thread.  The details are somewhat
  complicated, but the upshot is that after all Xen-based mitigations
  have been applied:

  * PV guests cannot spy on sibling threads
  * HVM guests can spy on sibling threads

  (NB that for purposes of this vulnerability, PVH and HVM guests are
  identical.  Whenever this comment refers to 'HVM', this includes PVH.)

  There are many possible mitigations to this, including disabling
  hyperthreading entirely.  But another solution would be:

  * Specify some cores as PV-only, others as PV or HVM
  * Allow HVM guests to only run on thread 0 of the "HVM-or-PV" cores
  * Allow PV guests to run on the above cores, as well as any thread of the PV-only cores.

  For example, suppose you had 16 threads across 8 cores (0-7).  You
  could specify 0-3 as PV-only, and 4-7 as HVM-or-PV.  Then you'd set
  the affinity of the HVM guests as follows (binary representation):

  0000000010101010

  And the affinity of the PV guests as follows:

  1111111110101010

  In order to make this easy, this patches introduces three "global affinity
  masks", placed in xl.conf:

      vm.cpumask
      vm.hvm.cpumask
      vm.pv.cpumask

  These are parsed just like the 'cpus' and 'cpus_soft' options in the
  per-domain xl configuration files.  The resulting mask is AND-ed with
  whatever mask results at the end of the xl configuration file.
  `vm.cpumask` would be applied to all guest types, `vm.hvm.cpumask`
  would be applied to HVM and PVH guest types, and `vm.pv.cpumask`
  would be applied to PV guest types.

  The idea would be that to implement the above mask across all your
  VMs, you'd simply add the following two lines to the configuration
  file:

      vm.hvm.cpumask=8,10,12,14
      vm.pv.cpumask=0-8,10,12,14

  See xl.conf manpage for details.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: George Dunlap <george.dunlap@citrix.com>
  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  (cherry picked from commit aa67b97ed34279c43a43d9ca46727b5746caa92e)

  PVH guest type in toolstack is not available in this version of Xen.
  Change code and manpage to cope.

  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  fd86a3c856108b6401c1fc12084fc41e18268956:x86/msr: Virtualise MSR_FLUSH_CMD for guests

  Guests (outside of the nested virt case, which isn't supported yet) don't need
  L1D_FLUSH for their L1TF mitigations, but offering/emulating MSR_FLUSH_CMD is
  easy and doesn't pose an issue for Xen.

  The MSR is offered to HVM guests only.  PV guests attempting to use it would
  trap for emulation, and the L1D cache would fill long before the return to
  guest context.  As such, PV guests can't make any use of the L1D_FLUSH
  functionality.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fd9823faf9df057a69a9a53c2e100691d3f4267c)
  e06752e2d6c483ca30c539d65b1c87fc3206f6a2:x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit 3563fc2b2731a63fd7e8372ab0f5cef205bf8477)
  13e85a6dbc1eeda4f95c0d3afcd205579eab5909:xl.conf: Add global affinity masks

  XSA-273 involves one hyperthread being able to use Spectre-like
  techniques to "spy" on another thread.  The details are somewhat
  complicated, but the upshot is that after all Xen-based mitigations
  have been applied:

  * PV guests cannot spy on sibling threads
  * HVM guests can spy on sibling threads

  (NB that for purposes of this vulnerability, PVH and HVM guests are
  identical.  Whenever this comment refers to 'HVM', this includes PVH.)

  There are many possible mitigations to this, including disabling
  hyperthreading entirely.  But another solution would be:

  * Specify some cores as PV-only, others as PV or HVM
  * Allow HVM guests to only run on thread 0 of the "HVM-or-PV" cores
  * Allow PV guests to run on the above cores, as well as any thread of the PV-only cores.

  For example, suppose you had 16 threads across 8 cores (0-7).  You
  could specify 0-3 as PV-only, and 4-7 as HVM-or-PV.  Then you'd set
  the affinity of the HVM guests as follows (binary representation):

  0000000010101010

  And the affinity of the PV guests as follows:

  1111111110101010

  In order to make this easy, this patches introduces three "global affinity
  masks", placed in xl.conf:

      vm.cpumask
      vm.hvm.cpumask
      vm.pv.cpumask

  These are parsed just like the 'cpus' and 'cpus_soft' options in the
  per-domain xl configuration files.  The resulting mask is AND-ed with
  whatever mask results at the end of the xl configuration file.
  `vm.cpumask` would be applied to all guest types, `vm.hvm.cpumask`
  would be applied to HVM and PVH guest types, and `vm.pv.cpumask`
  would be applied to PV guest types.

  The idea would be that to implement the above mask across all your
  VMs, you'd simply add the following two lines to the configuration
  file:

      vm.hvm.cpumask=8,10,12,14
      vm.pv.cpumask=0-8,10,12,14

  See xl.conf manpage for details.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: George Dunlap <george.dunlap@citrix.com>
  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  (cherry picked from commit aa67b97ed34279c43a43d9ca46727b5746caa92e)
  ef71d13e7f9cf197e17d004a0b30e8aeed09739b:x86/msr: Virtualise MSR_FLUSH_CMD for guests

  Guests (outside of the nested virt case, which isn't supported yet) don't need
  L1D_FLUSH for their L1TF mitigations, but offering/emulating MSR_FLUSH_CMD is
  easy and doesn't pose an issue for Xen.

  The MSR is offered to HVM guests only.  PV guests attempting to use it would
  trap for emulation, and the L1D cache would fill long before the return to
  guest context.  As such, PV guests can't make any use of the L1D_FLUSH
  functionality.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fd9823faf9df057a69a9a53c2e100691d3f4267c)
  80dd3f52bece1af7d6814b134c4460420f97d567:x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit 3563fc2b2731a63fd7e8372ab0f5cef205bf8477)
  d757c29ffe2e31b15397e43cd58da88b6318b654:xl.conf: Add global affinity masks

  XSA-273 involves one hyperthread being able to use Spectre-like
  techniques to "spy" on another thread.  The details are somewhat
  complicated, but the upshot is that after all Xen-based mitigations
  have been applied:

  * PV guests cannot spy on sibling threads
  * HVM guests can spy on sibling threads

  (NB that for purposes of this vulnerability, PVH and HVM guests are
  identical.  Whenever this comment refers to 'HVM', this includes PVH.)

  There are many possible mitigations to this, including disabling
  hyperthreading entirely.  But another solution would be:

  * Specify some cores as PV-only, others as PV or HVM
  * Allow HVM guests to only run on thread 0 of the "HVM-or-PV" cores
  * Allow PV guests to run on the above cores, as well as any thread of the PV-only cores.

  For example, suppose you had 16 threads across 8 cores (0-7).  You
  could specify 0-3 as PV-only, and 4-7 as HVM-or-PV.  Then you'd set
  the affinity of the HVM guests as follows (binary representation):

  0000000010101010

  And the affinity of the PV guests as follows:

  1111111110101010

  In order to make this easy, this patches introduces three "global affinity
  masks", placed in xl.conf:

      vm.cpumask
      vm.hvm.cpumask
      vm.pv.cpumask

  These are parsed just like the 'cpus' and 'cpus_soft' options in the
  per-domain xl configuration files.  The resulting mask is AND-ed with
  whatever mask results at the end of the xl configuration file.
  `vm.cpumask` would be applied to all guest types, `vm.hvm.cpumask`
  would be applied to HVM and PVH guest types, and `vm.pv.cpumask`
  would be applied to PV guest types.

  The idea would be that to implement the above mask across all your
  VMs, you'd simply add the following two lines to the configuration
  file:

      vm.hvm.cpumask=8,10,12,14
      vm.pv.cpumask=0-8,10,12,14

  See xl.conf manpage for details.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: George Dunlap <george.dunlap@citrix.com>
  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  (cherry picked from commit aa67b97ed34279c43a43d9ca46727b5746caa92e)
  007752fb9b85b9235fe2820677988c6408c583da:x86/msr: Virtualise MSR_FLUSH_CMD for guests

  Guests (outside of the nested virt case, which isn't supported yet) don't need
  L1D_FLUSH for their L1TF mitigations, but offering/emulating MSR_FLUSH_CMD is
  easy and doesn't pose an issue for Xen.

  The MSR is offered to HVM guests only.  PV guests attempting to use it would
  trap for emulation, and the L1D cache would fill long before the return to
  guest context.  As such, PV guests can't make any use of the L1D_FLUSH
  functionality.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fd9823faf9df057a69a9a53c2e100691d3f4267c)
  fb78137bb82d3d8bcac36430b8bc331008ee3826:x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit 3563fc2b2731a63fd7e8372ab0f5cef205bf8477)
  aa67b97ed34279c43a43d9ca46727b5746caa92e:xl.conf: Add global affinity masks

  XSA-273 involves one hyperthread being able to use Spectre-like
  techniques to "spy" on another thread.  The details are somewhat
  complicated, but the upshot is that after all Xen-based mitigations
  have been applied:

  * PV guests cannot spy on sibling threads
  * HVM guests can spy on sibling threads

  (NB that for purposes of this vulnerability, PVH and HVM guests are
  identical.  Whenever this comment refers to 'HVM', this includes PVH.)

  There are many possible mitigations to this, including disabling
  hyperthreading entirely.  But another solution would be:

  * Specify some cores as PV-only, others as PV or HVM
  * Allow HVM guests to only run on thread 0 of the "HVM-or-PV" cores
  * Allow PV guests to run on the above cores, as well as any thread of the PV-only cores.

  For example, suppose you had 16 threads across 8 cores (0-7).  You
  could specify 0-3 as PV-only, and 4-7 as HVM-or-PV.  Then you'd set
  the affinity of the HVM guests as follows (binary representation):

  0000000010101010

  And the affinity of the PV guests as follows:

  1111111110101010

  In order to make this easy, this patches introduces three "global affinity
  masks", placed in xl.conf:

      vm.cpumask
      vm.hvm.cpumask
      vm.pv.cpumask

  These are parsed just like the 'cpus' and 'cpus_soft' options in the
  per-domain xl configuration files.  The resulting mask is AND-ed with
  whatever mask results at the end of the xl configuration file.
  `vm.cpumask` would be applied to all guest types, `vm.hvm.cpumask`
  would be applied to HVM and PVH guest types, and `vm.pv.cpumask`
  would be applied to PV guest types.

  The idea would be that to implement the above mask across all your
  VMs, you'd simply add the following two lines to the configuration
  file:

      vm.hvm.cpumask=8,10,12,14
      vm.pv.cpumask=0-8,10,12,14

  See xl.conf manpage for details.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: George Dunlap <george.dunlap@citrix.com>
  Signed-off-by: Wei Liu <wei.liu2@citrix.com>
  fd9823faf9df057a69a9a53c2e100691d3f4267c:x86/msr: Virtualise MSR_FLUSH_CMD for guests

  Guests (outside of the nested virt case, which isn't supported yet) don't need
  L1D_FLUSH for their L1TF mitigations, but offering/emulating MSR_FLUSH_CMD is
  easy and doesn't pose an issue for Xen.

  The MSR is offered to HVM guests only.  PV guests attempting to use it would
  trap for emulation, and the L1D cache would fill long before the return to
  guest context.  As such, PV guests can't make any use of the L1D_FLUSH
  functionality.

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  3563fc2b2731a63fd7e8372ab0f5cef205bf8477:x86/spec-ctrl: CPUID/MSR definitions for L1D_FLUSH

  This is part of XSA-273 / CVE-2018-3646.

  Signed-off-by: Andrew Cooper <andrew.cooper3@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
impacted_packages:
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 5464d5f0c9cf030186ba88c246a3eb96f67277fc
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: fb78137bb82d3d8bcac36430b8bc331008ee3826
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: e06752e2d6c483ca30c539d65b1c87fc3206f6a2
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 9b8375a272ad02d8d0c229b3e3e7989e852734d8
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: c932ff17fc61a6d4c8041e077592800eec54e9f1
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: e1b03b03b199bd206c81286b4f51b6a681123eda
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: fd86a3c856108b6401c1fc12084fc41e18268956
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: ef1b64877424016c90400963adff056e9199e667
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: aa67b97ed34279c43a43d9ca46727b5746caa92e
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: b2449f57ae15e7899f37e58d6d4abf07ef29f95b
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 80dd3f52bece1af7d6814b134c4460420f97d567
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: f440b31efe42e988660812d33c2625bb061ae9ca
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: ef71d13e7f9cf197e17d004a0b30e8aeed09739b
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 3563fc2b2731a63fd7e8372ab0f5cef205bf8477
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 1f56fba4865d9acbd075df88ec67ee6358f11b63
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 007752fb9b85b9235fe2820677988c6408c583da
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: d95b5bb31e6d4361e356f0ff0853b6bb172a8b6a
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 13e85a6dbc1eeda4f95c0d3afcd205579eab5909
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: d757c29ffe2e31b15397e43cd58da88b6318b654
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: f05a33e327b87b30a14b6926e232c712ee7c4a1d
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 14f90aaef8d441cbdece5b74829e85e767fb196c
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: fd9823faf9df057a69a9a53c2e100691d3f4267c
    introduced_in_commits: []
severities: []
weaknesses: []
references:
  - url: https://github.com/xen-project/xen/tree/007752fb9b85b9235fe2820677988c6408c583da
    reference_type: commit
    reference_id: 007752fb9b85b9235fe2820677988c6408c583da
  - url: https://github.com/xen-project/xen/tree/13e85a6dbc1eeda4f95c0d3afcd205579eab5909
    reference_type: commit
    reference_id: 13e85a6dbc1eeda4f95c0d3afcd205579eab5909
  - url: https://github.com/xen-project/xen/tree/14f90aaef8d441cbdece5b74829e85e767fb196c
    reference_type: commit
    reference_id: 14f90aaef8d441cbdece5b74829e85e767fb196c
  - url: https://github.com/xen-project/xen/tree/1f56fba4865d9acbd075df88ec67ee6358f11b63
    reference_type: commit
    reference_id: 1f56fba4865d9acbd075df88ec67ee6358f11b63
  - url: https://github.com/xen-project/xen/tree/3563fc2b2731a63fd7e8372ab0f5cef205bf8477
    reference_type: commit
    reference_id: 3563fc2b2731a63fd7e8372ab0f5cef205bf8477
  - url: https://github.com/xen-project/xen/tree/5464d5f0c9cf030186ba88c246a3eb96f67277fc
    reference_type: commit
    reference_id: 5464d5f0c9cf030186ba88c246a3eb96f67277fc
  - url: https://github.com/xen-project/xen/tree/80dd3f52bece1af7d6814b134c4460420f97d567
    reference_type: commit
    reference_id: 80dd3f52bece1af7d6814b134c4460420f97d567
  - url: https://github.com/xen-project/xen/tree/9b8375a272ad02d8d0c229b3e3e7989e852734d8
    reference_type: commit
    reference_id: 9b8375a272ad02d8d0c229b3e3e7989e852734d8
  - url: https://github.com/xen-project/xen/tree/aa67b97ed34279c43a43d9ca46727b5746caa92e
    reference_type: commit
    reference_id: aa67b97ed34279c43a43d9ca46727b5746caa92e
  - url: https://github.com/xen-project/xen/tree/b2449f57ae15e7899f37e58d6d4abf07ef29f95b
    reference_type: commit
    reference_id: b2449f57ae15e7899f37e58d6d4abf07ef29f95b
  - url: https://github.com/xen-project/xen/tree/c932ff17fc61a6d4c8041e077592800eec54e9f1
    reference_type: commit
    reference_id: c932ff17fc61a6d4c8041e077592800eec54e9f1
  - url: https://github.com/xen-project/xen/tree/d757c29ffe2e31b15397e43cd58da88b6318b654
    reference_type: commit
    reference_id: d757c29ffe2e31b15397e43cd58da88b6318b654
  - url: https://github.com/xen-project/xen/tree/d95b5bb31e6d4361e356f0ff0853b6bb172a8b6a
    reference_type: commit
    reference_id: d95b5bb31e6d4361e356f0ff0853b6bb172a8b6a
  - url: https://github.com/xen-project/xen/tree/e06752e2d6c483ca30c539d65b1c87fc3206f6a2
    reference_type: commit
    reference_id: e06752e2d6c483ca30c539d65b1c87fc3206f6a2
  - url: https://github.com/xen-project/xen/tree/e1b03b03b199bd206c81286b4f51b6a681123eda
    reference_type: commit
    reference_id: e1b03b03b199bd206c81286b4f51b6a681123eda
  - url: https://github.com/xen-project/xen/tree/ef1b64877424016c90400963adff056e9199e667
    reference_type: commit
    reference_id: ef1b64877424016c90400963adff056e9199e667
  - url: https://github.com/xen-project/xen/tree/ef71d13e7f9cf197e17d004a0b30e8aeed09739b
    reference_type: commit
    reference_id: ef71d13e7f9cf197e17d004a0b30e8aeed09739b
  - url: https://github.com/xen-project/xen/tree/f05a33e327b87b30a14b6926e232c712ee7c4a1d
    reference_type: commit
    reference_id: f05a33e327b87b30a14b6926e232c712ee7c4a1d
  - url: https://github.com/xen-project/xen/tree/f440b31efe42e988660812d33c2625bb061ae9ca
    reference_type: commit
    reference_id: f440b31efe42e988660812d33c2625bb061ae9ca
  - url: https://github.com/xen-project/xen/tree/fb78137bb82d3d8bcac36430b8bc331008ee3826
    reference_type: commit
    reference_id: fb78137bb82d3d8bcac36430b8bc331008ee3826
  - url: https://github.com/xen-project/xen/tree/fd86a3c856108b6401c1fc12084fc41e18268956
    reference_type: commit
    reference_id: fd86a3c856108b6401c1fc12084fc41e18268956
  - url: https://github.com/xen-project/xen/tree/fd9823faf9df057a69a9a53c2e100691d3f4267c
    reference_type: commit
    reference_id: fd9823faf9df057a69a9a53c2e100691d3f4267c
