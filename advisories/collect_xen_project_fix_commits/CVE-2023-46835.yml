advisory_id: CVE-2023-46835
datasource_id: collect_xen_project_fix_commits/CVE-2023-46835
datasource_url: https://github.com/xen-project/xen
aliases: []
summary: |
  fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f:iommu/amd-vi: use correct level for quarantine domain page tables

  The current setup of the quarantine page tables assumes that the quarantine
  domain (dom_io) has been initialized with an address width of
  DEFAULT_DOMAIN_ADDRESS_WIDTH (48).

  However dom_io being a PV domain gets the AMD-Vi IOMMU page tables levels based
  on the maximum (hot pluggable) RAM address, and hence on systems with no RAM
  above the 512GB mark only 3 page-table levels are configured in the IOMMU.

  On systems without RAM above the 512GB boundary amd_iommu_quarantine_init()
  will setup page tables for the scratch page with 4 levels, while the IOMMU will
  be configured to use 3 levels only.  The page destined to be used as level 1,
  and to contain a directory of PTEs ends up being the address in a PTE itself,
  and thus level 1 page becomes the leaf page.  Without the level mismatch it's
  level 0 page that should be the leaf page instead.

  The level 1 page won't be used as such, and hence it's not possible to use it
  to gain access to other memory on the system.  However that page is not cleared
  in amd_iommu_quarantine_init() as part of re-initialization of the device
  quarantine page tables, and hence data on the level 1 page can be leaked
  between device usages.

  Fix this by making sure the paging levels setup by amd_iommu_quarantine_init()
  match the number configured on the IOMMUs.

  Note that IVMD regions are not affected by this issue, as those areas are
  mapped taking the configured paging levels into account.

  This is XSA-445 / CVE-2023-46835

  Fixes: ea38867831da ('x86 / iommu: set up a scratch page in the quarantine domain')
  Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  2d977ef880d709e868f36b9088d556d503b6e461:iommu/amd-vi: use correct level for quarantine domain page tables

  The current setup of the quarantine page tables assumes that the quarantine
  domain (dom_io) has been initialized with an address width of
  DEFAULT_DOMAIN_ADDRESS_WIDTH (48).

  However dom_io being a PV domain gets the AMD-Vi IOMMU page tables levels based
  on the maximum (hot pluggable) RAM address, and hence on systems with no RAM
  above the 512GB mark only 3 page-table levels are configured in the IOMMU.

  On systems without RAM above the 512GB boundary amd_iommu_quarantine_init()
  will setup page tables for the scratch page with 4 levels, while the IOMMU will
  be configured to use 3 levels only.  The page destined to be used as level 1,
  and to contain a directory of PTEs ends up being the address in a PTE itself,
  and thus level 1 page becomes the leaf page.  Without the level mismatch it's
  level 0 page that should be the leaf page instead.

  The level 1 page won't be used as such, and hence it's not possible to use it
  to gain access to other memory on the system.  However that page is not cleared
  in amd_iommu_quarantine_init() as part of re-initialization of the device
  quarantine page tables, and hence data on the level 1 page can be leaked
  between device usages.

  Fix this by making sure the paging levels setup by amd_iommu_quarantine_init()
  match the number configured on the IOMMUs.

  Note that IVMD regions are not affected by this issue, as those areas are
  mapped taking the configured paging levels into account.

  This is XSA-445 / CVE-2023-46835

  Fixes: ea38867831da ('x86 / iommu: set up a scratch page in the quarantine domain')
  Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f)
  1f5f515da0f694d97939f346c628a3b7b612d165:iommu/amd-vi: use correct level for quarantine domain page tables

  The current setup of the quarantine page tables assumes that the quarantine
  domain (dom_io) has been initialized with an address width of
  DEFAULT_DOMAIN_ADDRESS_WIDTH (48).

  However dom_io being a PV domain gets the AMD-Vi IOMMU page tables levels based
  on the maximum (hot pluggable) RAM address, and hence on systems with no RAM
  above the 512GB mark only 3 page-table levels are configured in the IOMMU.

  On systems without RAM above the 512GB boundary amd_iommu_quarantine_init()
  will setup page tables for the scratch page with 4 levels, while the IOMMU will
  be configured to use 3 levels only.  The page destined to be used as level 1,
  and to contain a directory of PTEs ends up being the address in a PTE itself,
  and thus level 1 page becomes the leaf page.  Without the level mismatch it's
  level 0 page that should be the leaf page instead.

  The level 1 page won't be used as such, and hence it's not possible to use it
  to gain access to other memory on the system.  However that page is not cleared
  in amd_iommu_quarantine_init() as part of re-initialization of the device
  quarantine page tables, and hence data on the level 1 page can be leaked
  between device usages.

  Fix this by making sure the paging levels setup by amd_iommu_quarantine_init()
  match the number configured on the IOMMUs.

  Note that IVMD regions are not affected by this issue, as those areas are
  mapped taking the configured paging levels into account.

  This is XSA-445 / CVE-2023-46835

  Fixes: ea38867831da ('x86 / iommu: set up a scratch page in the quarantine domain')
  Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f)
  6fea3835d91aaa048f66036c34c25532e2dd2b67:iommu/amd-vi: use correct level for quarantine domain page tables

  The current setup of the quarantine page tables assumes that the quarantine
  domain (dom_io) has been initialized with an address width of
  DEFAULT_DOMAIN_ADDRESS_WIDTH (48).

  However dom_io being a PV domain gets the AMD-Vi IOMMU page tables levels based
  on the maximum (hot pluggable) RAM address, and hence on systems with no RAM
  above the 512GB mark only 3 page-table levels are configured in the IOMMU.

  On systems without RAM above the 512GB boundary amd_iommu_quarantine_init()
  will setup page tables for the scratch page with 4 levels, while the IOMMU will
  be configured to use 3 levels only.  The page destined to be used as level 1,
  and to contain a directory of PTEs ends up being the address in a PTE itself,
  and thus level 1 page becomes the leaf page.  Without the level mismatch it's
  level 0 page that should be the leaf page instead.

  The level 1 page won't be used as such, and hence it's not possible to use it
  to gain access to other memory on the system.  However that page is not cleared
  in amd_iommu_quarantine_init() as part of re-initialization of the device
  quarantine page tables, and hence data on the level 1 page can be leaked
  between device usages.

  Fix this by making sure the paging levels setup by amd_iommu_quarantine_init()
  match the number configured on the IOMMUs.

  Note that IVMD regions are not affected by this issue, as those areas are
  mapped taking the configured paging levels into account.

  This is XSA-445 / CVE-2023-46835

  Fixes: ea38867831da ('x86 / iommu: set up a scratch page in the quarantine domain')
  Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f)
  b7a1e1053977c986dc7b5ca6fe435ca94bc45f6c:iommu/amd-vi: use correct level for quarantine domain page tables

  The current setup of the quarantine page tables assumes that the quarantine
  domain (dom_io) has been initialized with an address width of
  DEFAULT_DOMAIN_ADDRESS_WIDTH (48).

  However dom_io being a PV domain gets the AMD-Vi IOMMU page tables levels based
  on the maximum (hot pluggable) RAM address, and hence on systems with no RAM
  above the 512GB mark only 3 page-table levels are configured in the IOMMU.

  On systems without RAM above the 512GB boundary amd_iommu_quarantine_init()
  will setup page tables for the scratch page with 4 levels, while the IOMMU will
  be configured to use 3 levels only.  The page destined to be used as level 1,
  and to contain a directory of PTEs ends up being the address in a PTE itself,
  and thus level 1 page becomes the leaf page.  Without the level mismatch it's
  level 0 page that should be the leaf page instead.

  The level 1 page won't be used as such, and hence it's not possible to use it
  to gain access to other memory on the system.  However that page is not cleared
  in amd_iommu_quarantine_init() as part of re-initialization of the device
  quarantine page tables, and hence data on the level 1 page can be leaked
  between device usages.

  Fix this by making sure the paging levels setup by amd_iommu_quarantine_init()
  match the number configured on the IOMMUs.

  Note that IVMD regions are not affected by this issue, as those areas are
  mapped taking the configured paging levels into account.

  This is XSA-445 / CVE-2023-46835

  Fixes: ea38867831da ('x86 / iommu: set up a scratch page in the quarantine domain')
  Signed-off-by: Roger Pau Monné <roger.pau@citrix.com>
  Reviewed-by: Jan Beulich <jbeulich@suse.com>
  (cherry picked from commit fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f)
impacted_packages:
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 2d977ef880d709e868f36b9088d556d503b6e461
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 6fea3835d91aaa048f66036c34c25532e2dd2b67
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: b7a1e1053977c986dc7b5ca6fe435ca94bc45f6c
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 1f5f515da0f694d97939f346c628a3b7b612d165
    introduced_in_commits: []
severities: []
weaknesses: []
references:
  - url: https://github.com/xen-project/xen/tree/1f5f515da0f694d97939f346c628a3b7b612d165
    reference_type: commit
    reference_id: 1f5f515da0f694d97939f346c628a3b7b612d165
  - url: https://github.com/xen-project/xen/tree/2d977ef880d709e868f36b9088d556d503b6e461
    reference_type: commit
    reference_id: 2d977ef880d709e868f36b9088d556d503b6e461
  - url: https://github.com/xen-project/xen/tree/6fea3835d91aaa048f66036c34c25532e2dd2b67
    reference_type: commit
    reference_id: 6fea3835d91aaa048f66036c34c25532e2dd2b67
  - url: https://github.com/xen-project/xen/tree/b7a1e1053977c986dc7b5ca6fe435ca94bc45f6c
    reference_type: commit
    reference_id: b7a1e1053977c986dc7b5ca6fe435ca94bc45f6c
  - url: https://github.com/xen-project/xen/tree/fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f
    reference_type: commit
    reference_id: fe1e4668b373ec4c1e5602e75905a9fa8cc2be3f
