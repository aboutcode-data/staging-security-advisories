advisory_id: CVE-2022-42332
datasource_id: collect_xen_project_fix_commits/CVE-2022-42332
datasource_url: https://github.com/xen-project/xen
aliases: []
summary: |
  254663bec2bcae8b0a658f3db7a4dfb5ab6a0c83:x86/shadow: account for log-dirty mode when pre-allocating

  Pre-allocation is intended to ensure that in the course of constructing
  or updating shadows there won't be any risk of just made shadows or
  shadows being acted upon can disappear under our feet. The amount of
  pages pre-allocated then, however, needs to account for all possible
  subsequent allocations. While the use in sh_page_fault() accounts for
  all shadows which may need making, so far it didn't account for
  allocations coming from log-dirty tracking (which piggybacks onto the
  P2M allocation functions).

  Since shadow_prealloc() takes a count of shadows (or other data
  structures) rather than a count of pages, putting the adjustment at the
  call site of this function won't work very well: We simply can't express
  the correct count that way in all cases. Instead take care of this in
  the function itself, by "snooping" for L1 type requests. (While not
  applicable right now, future new request sites of L1 tables would then
  also be covered right away.)

  It is relevant to note here that pre-allocations like the one done from
  shadow_alloc_p2m_page() are benign when they fall in the "scope" of an
  earlier pre-alloc which already included that count: The inner call will
  simply find enough pages available then; it'll bail right away.

  This is CVE-2022-42332 / XSA-427.

  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Tim Deegan <tim@xen.org>
  (cherry picked from commit 91767a71061035ae42be93de495cd976f863a41a)
  3a0b7fb38a3e40fcf82c10980775f0fecab667b5:x86/shadow: account for log-dirty mode when pre-allocating

  Pre-allocation is intended to ensure that in the course of constructing
  or updating shadows there won't be any risk of just made shadows or
  shadows being acted upon can disappear under our feet. The amount of
  pages pre-allocated then, however, needs to account for all possible
  subsequent allocations. While the use in sh_page_fault() accounts for
  all shadows which may need making, so far it didn't account for
  allocations coming from log-dirty tracking (which piggybacks onto the
  P2M allocation functions).

  Since shadow_prealloc() takes a count of shadows (or other data
  structures) rather than a count of pages, putting the adjustment at the
  call site of this function won't work very well: We simply can't express
  the correct count that way in all cases. Instead take care of this in
  the function itself, by "snooping" for L1 type requests. (While not
  applicable right now, future new request sites of L1 tables would then
  also be covered right away.)

  It is relevant to note here that pre-allocations like the one done from
  shadow_alloc_p2m_page() are benign when they fall in the "scope" of an
  earlier pre-alloc which already included that count: The inner call will
  simply find enough pages available then; it'll bail right away.

  This is CVE-2022-42332 / XSA-427.

  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Tim Deegan <tim@xen.org>
  (cherry picked from commit 91767a71061035ae42be93de495cd976f863a41a)
  b0d6684ee58f7252940f5a62e4b85bdc56307eef:x86/shadow: account for log-dirty mode when pre-allocating

  Pre-allocation is intended to ensure that in the course of constructing
  or updating shadows there won't be any risk of just made shadows or
  shadows being acted upon can disappear under our feet. The amount of
  pages pre-allocated then, however, needs to account for all possible
  subsequent allocations. While the use in sh_page_fault() accounts for
  all shadows which may need making, so far it didn't account for
  allocations coming from log-dirty tracking (which piggybacks onto the
  P2M allocation functions).

  Since shadow_prealloc() takes a count of shadows (or other data
  structures) rather than a count of pages, putting the adjustment at the
  call site of this function won't work very well: We simply can't express
  the correct count that way in all cases. Instead take care of this in
  the function itself, by "snooping" for L1 type requests. (While not
  applicable right now, future new request sites of L1 tables would then
  also be covered right away.)

  It is relevant to note here that pre-allocations like the one done from
  shadow_alloc_p2m_page() are benign when they fall in the "scope" of an
  earlier pre-alloc which already included that count: The inner call will
  simply find enough pages available then; it'll bail right away.

  This is CVE-2022-42332 / XSA-427.

  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Tim Deegan <tim@xen.org>
  (cherry picked from commit 91767a71061035ae42be93de495cd976f863a41a)
  f8f8f07880d3817fc7b0472420eca9fecaa55358:x86/shadow: account for log-dirty mode when pre-allocating

  Pre-allocation is intended to ensure that in the course of constructing
  or updating shadows there won't be any risk of just made shadows or
  shadows being acted upon can disappear under our feet. The amount of
  pages pre-allocated then, however, needs to account for all possible
  subsequent allocations. While the use in sh_page_fault() accounts for
  all shadows which may need making, so far it didn't account for
  allocations coming from log-dirty tracking (which piggybacks onto the
  P2M allocation functions).

  Since shadow_prealloc() takes a count of shadows (or other data
  structures) rather than a count of pages, putting the adjustment at the
  call site of this function won't work very well: We simply can't express
  the correct count that way in all cases. Instead take care of this in
  the function itself, by "snooping" for L1 type requests. (While not
  applicable right now, future new request sites of L1 tables would then
  also be covered right away.)

  It is relevant to note here that pre-allocations like the one done from
  shadow_alloc_p2m_page() are benign when they fall in the "scope" of an
  earlier pre-alloc which already included that count: The inner call will
  simply find enough pages available then; it'll bail right away.

  This is CVE-2022-42332 / XSA-427.

  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Tim Deegan <tim@xen.org>
  (cherry picked from commit 91767a71061035ae42be93de495cd976f863a41a)
  33fb3a6612235a23b30df556bf92b52a3450c340:x86/shadow: account for log-dirty mode when pre-allocating

  Pre-allocation is intended to ensure that in the course of constructing
  or updating shadows there won't be any risk of just made shadows or
  shadows being acted upon can disappear under our feet. The amount of
  pages pre-allocated then, however, needs to account for all possible
  subsequent allocations. While the use in sh_page_fault() accounts for
  all shadows which may need making, so far it didn't account for
  allocations coming from log-dirty tracking (which piggybacks onto the
  P2M allocation functions).

  Since shadow_prealloc() takes a count of shadows (or other data
  structures) rather than a count of pages, putting the adjustment at the
  call site of this function won't work very well: We simply can't express
  the correct count that way in all cases. Instead take care of this in
  the function itself, by "snooping" for L1 type requests. (While not
  applicable right now, future new request sites of L1 tables would then
  also be covered right away.)

  It is relevant to note here that pre-allocations like the one done from
  shadow_alloc_p2m_page() are benign when they fall in the "scope" of an
  earlier pre-alloc which already included that count: The inner call will
  simply find enough pages available then; it'll bail right away.

  This is CVE-2022-42332 / XSA-427.

  Signed-off-by: Jan Beulich <jbeulich@suse.com>
  Reviewed-by: Tim Deegan <tim@xen.org>
impacted_packages:
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 254663bec2bcae8b0a658f3db7a4dfb5ab6a0c83
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 3a0b7fb38a3e40fcf82c10980775f0fecab667b5
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: b0d6684ee58f7252940f5a62e4b85bdc56307eef
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: f8f8f07880d3817fc7b0472420eca9fecaa55358
    introduced_in_commits: []
  - purl: pkg:github/xen-project/xen
    affected_versions:
    fixed_versions:
    fixed_in_commits:
      - vcs_url: https://github.com/xen-project/xen
        commit: 33fb3a6612235a23b30df556bf92b52a3450c340
    introduced_in_commits: []
severities: []
weaknesses: []
references:
  - url: https://github.com/xen-project/xen/tree/254663bec2bcae8b0a658f3db7a4dfb5ab6a0c83
    reference_type: commit
    reference_id: 254663bec2bcae8b0a658f3db7a4dfb5ab6a0c83
  - url: https://github.com/xen-project/xen/tree/33fb3a6612235a23b30df556bf92b52a3450c340
    reference_type: commit
    reference_id: 33fb3a6612235a23b30df556bf92b52a3450c340
  - url: https://github.com/xen-project/xen/tree/3a0b7fb38a3e40fcf82c10980775f0fecab667b5
    reference_type: commit
    reference_id: 3a0b7fb38a3e40fcf82c10980775f0fecab667b5
  - url: https://github.com/xen-project/xen/tree/b0d6684ee58f7252940f5a62e4b85bdc56307eef
    reference_type: commit
    reference_id: b0d6684ee58f7252940f5a62e4b85bdc56307eef
  - url: https://github.com/xen-project/xen/tree/f8f8f07880d3817fc7b0472420eca9fecaa55358
    reference_type: commit
    reference_id: f8f8f07880d3817fc7b0472420eca9fecaa55358
