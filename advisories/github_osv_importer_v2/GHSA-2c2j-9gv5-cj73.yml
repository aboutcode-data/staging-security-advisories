advisory_id: GHSA-2c2j-9gv5-cj73
datasource_id: github_osv_importer_v2/GHSA-2c2j-9gv5-cj73
datasource_url: https://github.com/github/advisory-database/blob/main/advisories/github-reviewed/2025/07/GHSA-2c2j-9gv5-cj73/GHSA-2c2j-9gv5-cj73.json
aliases:
  - CVE-2025-54121
summary: |
  Starlette has possible denial-of-service vector when parsing large files in multipart forms
  ### Summary
  When parsing a multi-part form with large files (greater than the [default max spool size](https://github.com/encode/starlette/blob/fa5355442753f794965ae1af0f87f9fec1b9a3de/starlette/formparsers.py#L126)) `starlette` will block the main thread to roll the file over to disk. This blocks the event thread which means we can't accept new connections.

  ### Details
  Please see this discussion for details: https://github.com/encode/starlette/discussions/2927#discussioncomment-13721403. In summary the following UploadFile code (copied from [here](https://github.com/encode/starlette/blob/fa5355442753f794965ae1af0f87f9fec1b9a3de/starlette/datastructures.py#L436C5-L447C14)) has a minor bug. Instead of just checking for `self._in_memory` we should also check if the additional bytes will cause a rollover.

  ```python

      @property
      def _in_memory(self) -> bool:
          # check for SpooledTemporaryFile._rolled
          rolled_to_disk = getattr(self.file, "_rolled", True)
          return not rolled_to_disk

      async def write(self, data: bytes) -> None:
          if self.size is not None:
              self.size += len(data)

          if self._in_memory:
              self.file.write(data)
          else:
              await run_in_threadpool(self.file.write, data)
  ```

  I have already created a PR which fixes the problem: https://github.com/encode/starlette/pull/2962


  ### PoC
  See the discussion [here](https://github.com/encode/starlette/discussions/2927#discussioncomment-13721403) for steps on how to reproduce.

  ### Impact
  To be honest, very low and not many users will be impacted. Parsing large forms is already CPU intensive so the additional IO block doesn't slow down `starlette` that much on systems with modern HDDs/SSDs. If someone is running on tape they might see a greater impact.
impacted_packages:
  - purl: pkg:pypi/starlette
    affected_versions: vers:pypi/<0.47.2
    fixed_versions: vers:pypi/0.47.2
    fixed_in_commits: []
    introduced_in_commits: []
severities:
  - score: '5.3'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:N/I:N/A:L
    published_at: None
    url: https://github.com/github/advisory-database/blob/main/advisories/github-reviewed/2025/07/GHSA-2c2j-9gv5-cj73/GHSA-2c2j-9gv5-cj73.json
  - score: MODERATE
    scoring_system: generic_textual
    scoring_elements:
    published_at: None
    url:
weaknesses:
  - CWE-770
references:
  - url: https://github.com/encode/starlette
    reference_type:
    reference_id:
  - url: https://github.com/encode/starlette/blob/fa5355442753f794965ae1af0f87f9fec1b9a3de/starlette/datastructures.py#L436C5-L447C14
    reference_type:
    reference_id:
  - url: https://github.com/encode/starlette/commit/9f7ec2eb512fcc3fe90b43cb9dd9e1d08696bec1
    reference_type:
    reference_id:
  - url: https://github.com/encode/starlette/discussions/2927#discussioncomment-13721403
    reference_type:
    reference_id:
  - url: https://github.com/encode/starlette/security/advisories/GHSA-2c2j-9gv5-cj73
    reference_type:
    reference_id:
  - url: https://nvd.nist.gov/vuln/detail/CVE-2025-54121
    reference_type:
    reference_id: CVE-2025-54121
