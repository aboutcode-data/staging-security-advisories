advisory_id: GHSA-6qv9-48xg-fc7f
datasource_id: github_osv_importer_v2/GHSA-6qv9-48xg-fc7f
datasource_url: https://github.com/github/advisory-database/blob/main/advisories/github-reviewed/2025/11/GHSA-6qv9-48xg-fc7f/GHSA-6qv9-48xg-fc7f.json
aliases:
  - CVE-2025-65106
summary: |
  LangChain Vulnerable to Template Injection via Attribute Access in Prompt Templates
  ## Context

  A template injection vulnerability exists in LangChain's prompt template system that allows attackers to access Python object internals through template syntax. This vulnerability affects applications that accept **untrusted template strings** (not just template variables) in `ChatPromptTemplate` and related prompt template classes.

  Templates allow attribute access (`.`) and indexing (`[]`) but not method invocation (`()`).

  The combination of attribute access and indexing may enable exploitation depending on which objects are passed to templates. When template variables are simple strings (the common case), the impact is limited. However, when using `MessagesPlaceholder` with chat message objects, attackers can traverse through object attributes and dictionary lookups (e.g., `__globals__`) to reach sensitive data such as environment variables.

  The vulnerability specifically requires that applications accept **template strings** (the structure) from untrusted sources, not just **template variables** (the data). Most applications either do not use templates or else use hardcoded templates and are not vulnerable.

  ## Affected Components

  - `langchain-core` package
  - Template formats:
    - F-string templates (`template_format="f-string"`) - **Vulnerability fixed**
    - Mustache templates (`template_format="mustache"`) - **Defensive hardening**
    - Jinja2 templates (`template_format="jinja2"`) - **Defensive hardening**

  ### Impact
  Attackers who can control template strings (not just template variables) can:
  - Access Python object attributes and internal properties via attribute traversal
  - Extract sensitive information from object internals (e.g., `__class__`, `__globals__`)
  - Potentially escalate to more severe attacks depending on the objects passed to templates

  ### Attack Vectors

  #### 1. F-string Template Injection
  **Before Fix:**
  ```python
  from langchain_core.prompts import ChatPromptTemplate

  malicious_template = ChatPromptTemplate.from_messages(
      [("human", "{msg.__class__.__name__}")],
      template_format="f-string"
  )

  # Note that this requires passing a placeholder variable for "msg.__class__.__name__".
  result = malicious_template.invoke({"msg": "foo", "msg.__class__.__name__": "safe_placeholder"})
  # Previously returned
  # >>> result.messages[0].content
  # >>> 'str'
  ```

  #### 2. Mustache Template Injection
  **Before Fix:**
  ```python
  from langchain_core.prompts import ChatPromptTemplate
  from langchain_core.messages import HumanMessage

  msg = HumanMessage("Hello")

  # Attacker controls the template string
  malicious_template = ChatPromptTemplate.from_messages(
      [("human", "{{question.__class__.__name__}}")],
      template_format="mustache"
  )

  result = malicious_template.invoke({"question": msg})
  # Previously returned: "HumanMessage" (getattr() exposed internals)
  ```

  #### 3. Jinja2 Template Injection
  **Before Fix:**
  ```python
  from langchain_core.prompts import ChatPromptTemplate
  from langchain_core.messages import HumanMessage

  msg = HumanMessage("Hello")

  # Attacker controls the template string
  malicious_template = ChatPromptTemplate.from_messages(
      [("human", "{{question.parse_raw}}")],
      template_format="jinja2"
  )

  result = malicious_template.invoke({"question": msg})
  # Could access non-dunder attributes/methods on objects
  ```

  ### Root Cause

  1. **F-string templates**: The implementation used Python's `string.Formatter().parse()` to extract variable names from template strings. This method returns the complete field expression, including attribute access syntax:
       ```python
       from string import Formatter

       template = "{msg.__class__} and {x}"
       print([var_name for (_, var_name, _, _) in Formatter().parse(template)])
       # Returns: ['msg.__class__', 'x']
      ```
       The extracted names were not validated to ensure they were simple identifiers. As a result, template strings containing attribute traversal and indexing expressions (e.g., `{obj.__class__.__name__}` or `{obj.method.__globals__[os]}`) were accepted and subsequently evaluated during formatting. While f-string templates do not support method calls with `()`, they do support `[]` indexing, which could allow traversal through dictionaries like `__globals__` to reach sensitive objects.
  2. **Mustache templates**: By design, used `getattr()` as a fallback to support accessing attributes on objects (e.g., `{{user.name}}` on a User object). However, we decided to restrict this to simpler primitives that subclass dict, list, and tuple types as defensive hardening, since untrusted templates could exploit attribute access to reach internal properties like class on arbitrary objects
  3. **Jinja2 templates**: Jinja2's default `SandboxedEnvironment` blocks dunder attributes (e.g., `__class__`) but permits access to other attributes and methods on objects. While Jinja2 templates in LangChain are typically used with trusted template strings, as a defense-in-depth measure, we've restricted the environment to block all attribute and method access on objects
     passed to templates.


  ## Who Is Affected?

  ### High Risk Scenarios
  You are affected if your application:
  - Accepts template strings from untrusted sources (user input, external APIs, databases)
  - Dynamically constructs prompt templates based on user-provided patterns
  - Allows users to customize or create prompt templates

  **Example vulnerable code:**
  ```python
  # User controls the template string itself
  user_template_string = request.json.get("template")  # DANGEROUS

  prompt = ChatPromptTemplate.from_messages(
      [("human", user_template_string)],
      template_format="mustache"
  )

  result = prompt.invoke({"data": sensitive_object})
  ```

  ### Low/No Risk Scenarios
  You are **NOT** affected if:
  - Template strings are hardcoded in your application code
  - Template strings come only from trusted, controlled sources
  - Users can only provide **values** for template variables, not the template structure itself

  **Example safe code:**
  ```python
  # Template is hardcoded - users only control variables
  prompt = ChatPromptTemplate.from_messages(
      [("human", "User question: {question}")],  # SAFE
      template_format="f-string"
  )

  # User input only fills the 'question' variable
  result = prompt.invoke({"question": user_input})
  ```

  ## The Fix

  ### F-string Templates
  F-string templates had a clear vulnerability where attribute access syntax was exploitable. We've added strict validation to prevent this:

  - Added validation to enforce that variable names must be valid Python identifiers
  - Rejects syntax like `{obj.attr}`, `{obj[0]}`, or `{obj.__class__}`
  - Only allows simple variable names: `{variable_name}`

  ```python
  # After fix - these are rejected at template creation time
  ChatPromptTemplate.from_messages(
      [("human", "{msg.__class__}")],  # ValueError: Invalid variable name
      template_format="f-string"
  )
  ```

  ### Mustache Templates (Defensive Hardening)
  As defensive hardening, we've restricted what Mustache templates support to reduce the attack surface:

  - Replaced `getattr()` fallback with strict type checking
  - Only allows traversal into `dict`, `list`, and `tuple` types
  - Blocks attribute access on arbitrary Python objects

  ```python
  # After hardening - attribute access returns empty string
  prompt = ChatPromptTemplate.from_messages(
      [("human", "{{msg.__class__}}")],
      template_format="mustache"
  )
  result = prompt.invoke({"msg": HumanMessage("test")})
  # Returns: "" (access blocked)
  ```

  ### Jinja2 Templates (Defensive Hardening)
  As defensive hardening, we've significantly restricted Jinja2 template capabilities:

  - Introduced `_RestrictedSandboxedEnvironment` that blocks **ALL** attribute/method access
  - Only allows simple variable lookups from the context dictionary
  - Raises `SecurityError` on any attribute access attempt

  ```python
  # After hardening - all attribute access is blocked
  prompt = ChatPromptTemplate.from_messages(
      [("human", "{{msg.content}}")],
      template_format="jinja2"
  )
  # Raises SecurityError: Access to attributes is not allowed
  ```

  **Important Recommendation**: Due to the expressiveness of Jinja2 and the difficulty of fully sandboxing it, **we recommend reserving Jinja2 templates for trusted sources only**. If you need to accept template strings from untrusted users, use f-string or mustache templates with the new restrictions instead.

  While we've hardened the Jinja2 implementation, the nature of templating engines makes comprehensive sandboxing challenging. The safest approach is to only use Jinja2 templates when you control the template source.

  **Important Reminder**: Many applications do not need prompt templates. Templates are useful for variable substitution and dynamic logic (if statements, loops, conditionals). However, if you're building a chatbot or conversational application, you can often work directly with message objects (e.g., `HumanMessage`, `AIMessage`, `ToolMessage`) without templates. Direct message construction avoids template-related security concerns entirely.

  ## Remediation

  ### Immediate Actions

  1. **Audit your code** for any locations where template strings come from untrusted sources
  2. **Update to the patched version** of `langchain-core`
  3. **Review template usage** to ensure separation between template structure and user data

  ### Best Practices

  - **Consider if you need templates at all** - Many applications can work directly with message objects (`HumanMessage`, `AIMessage`, etc.) without templates## Context

  A template injection vulnerability exists in LangChain's prompt template system that allows attackers to access Python object internals through template syntax. This vulnerability affects applications that accept **untrusted template strings** (not just template variables) in `ChatPromptTemplate` and related prompt template classes.

  Templates allow attribute access (`.`) and indexing (`[]`) but not method invocation (`()`).

  The combination of attribute access and indexing may enable exploitation depending on which objects are passed to templates. When template variables are simple strings (the common case), the impact is limited. However, when using `MessagesPlaceholder` with chat message objects, attackers can traverse through object attributes and dictionary lookups (e.g., `__globals__`) to reach sensitive data such as environment variables.

  The vulnerability specifically requires that applications accept **template strings** (the structure) from untrusted sources, not just **template variables** (the data). Most applications either do not use templates or else use hardcoded templates and are not vulnerable.

  ## Affected Components

  - `langchain-core` package
  - Template formats:
    - F-string templates (`template_format="f-string"`) - **Vulnerability fixed**
    - Mustache templates (`template_format="mustache"`) - **Defensive hardening**
    - Jinja2 templates (`template_format="jinja2"`) - **Defensive hardening**

  ### Impact
  Attackers who can control template strings (not just template variables) can:
  - Access Python object attributes and internal properties via attribute traversal
  - Extract sensitive information from object internals (e.g., `__class__`, `__globals__`)
  - Potentially escalate to more severe attacks depending on the objects passed to templates

  ### Attack Vectors

  #### 1. F-string Template Injection
  **Before Fix:**
  ```python
  from langchain_core.prompts import ChatPromptTemplate

  malicious_template = ChatPromptTemplate.from_messages(
      [("human", "{msg.__class__.__name__}")],
      template_format="f-string"
  )

  # Note that this requires passing a placeholder variable for "msg.__class__.__name__".
  result = malicious_template.invoke({"msg": "foo", "msg.__class__.__name__": "safe_placeholder"})
  # Previously returned
  # >>> result.messages[0].content
  # >>> 'str'
  ```

  #### 2. Mustache Template Injection
  **Before Fix:**
  ```python
  from langchain_core.prompts import ChatPromptTemplate
  from langchain_core.messages import HumanMessage

  msg = HumanMessage("Hello")

  # Attacker controls the template string
  malicious_template = ChatPromptTemplate.from_messages(
      [("human", "{{question.__class__.__name__}}")],
      template_format="mustache"
  )

  result = malicious_template.invoke({"question": msg})
  # Previously returned: "HumanMessage" (getattr() exposed internals)
  ```

  #### 3. Jinja2 Template Injection
  **Before Fix:**
  ```python
  from langchain_core.prompts import ChatPromptTemplate
  from langchain_core.messages import HumanMessage

  msg = HumanMessage("Hello")

  # Attacker controls the template string
  malicious_template = ChatPromptTemplate.from_messages(
      [("human", "{{question.parse_raw}}")],
      template_format="jinja2"
  )

  result = malicious_template.invoke({"question": msg})
  # Could access non-dunder attributes/methods on objects
  ```

  ### Root Cause

  1. **F-string templates**: The implementation used Python's `string.Formatter().parse()` to extract variable names from template strings. This method returns the complete field expression, including attribute access syntax:
       ```python
       from string import Formatter

       template = "{msg.__class__} and {x}"
       print([var_name for (_, var_name, _, _) in Formatter().parse(template)])
       # Returns: ['msg.__class__', 'x']
      ```
       The extracted names were not validated to ensure they were simple identifiers. As a result, template strings containing attribute traversal and indexing expressions (e.g., `{obj.__class__.__name__}` or `{obj.method.__globals__[os]}`) were accepted and subsequently evaluated during formatting. While f-string templates do not support method calls with `()`, they do support `[]` indexing, which could allow traversal through dictionaries like `__globals__` to reach sensitive objects.
  2. **Mustache templates**: By design, used `getattr()` as a fallback to support accessing attributes on objects (e.g., `{{user.name}}` on a User object). However, we decided to restrict this to simpler primitives that subclass dict, list, and tuple types as defensive hardening, since untrusted templates could exploit attribute access to reach internal properties like class on arbitrary objects
  3. **Jinja2 templates**: Jinja2's default `SandboxedEnvironment` blocks dunder attributes (e.g., `__class__`) but permits access to other attributes and methods on objects. While Jinja2 templates in LangChain are typically used with trusted template strings, as a defense-in-depth measure, we've restricted the environment to block all attribute and method access on objects
     passed to templates.


  ## Who Is Affected?

  ### High Risk Scenarios
  You are affected if your application:
  - Accepts template strings from untrusted sources (user input, external APIs, databases)
  - Dynamically constructs prompt templates based on user-provided patterns
  - Allows users to customize or create prompt templates

  **Example vulnerable code:**
  ```python
  # User controls the template string itself
  user_template_string = request.json.get("template")  # DANGEROUS

  prompt = ChatPromptTemplate.from_messages(
      [("human", user_template_string)],
      template_format="mustache"
  )

  result = prompt.invoke({"data": sensitive_object})
  ```

  ### Low/No Risk Scenarios
  You are **NOT** affected if:
  - Template strings are hardcoded in your application code
  - Template strings come only from trusted, controlled sources
  - Users can only provide **values** for template variables, not the template structure itself

  **Example safe code:**
  ```python
  # Template is hardcoded - users only control variables
  prompt = ChatPromptTemplate.from_messages(
      [("human", "User question: {question}")],  # SAFE
      template_format="f-string"
  )

  # User input only fills the 'question' variable
  result = prompt.invoke({"question": user_input})
  ```

  ## The Fix

  ### F-string Templates
  F-string templates had a clear vulnerability where attribute access syntax was exploitable. We've added strict validation to prevent this:

  - Added validation to enforce that variable names must be valid Python identifiers
  - Rejects syntax like `{obj.attr}`, `{obj[0]}`, or `{obj.__class__}`
  - Only allows simple variable names: `{variable_name}`

  ```python
  # After fix - these are rejected at template creation time
  ChatPromptTemplate.from_messages(
      [("human", "{msg.__class__}")],  # ValueError: Invalid variable name
      template_format="f-string"
  )
  ```

  ### Mustache Templates (Defensive Hardening)
  As defensive hardening, we've restricted what Mustache templates support to reduce the attack surface:

  - Replaced `getattr()` fallback with strict type checking
  - Only allows traversal into `dict`, `list`, and `tuple` types
  - Blocks attribute access on arbitrary Python objects

  ```python
  # After hardening - attribute access returns empty string
  prompt = ChatPromptTemplate.from_messages(
      [("human", "{{msg.__class__}}")],
      template_format="mustache"
  )
  result = prompt.invoke({"msg": HumanMessage("test")})
  # Returns: "" (access blocked)
  ```

  ### Jinja2 Templates (Defensive Hardening)
  As defensive hardening, we've significantly restricted Jinja2 template capabilities:

  - Introduced `_RestrictedSandboxedEnvironment` that blocks **ALL** attribute/method access
  - Only allows simple variable lookups from the context dictionary
  - Raises `SecurityError` on any attribute access attempt

  ```python
  # After hardening - all attribute access is blocked
  prompt = ChatPromptTemplate.from_messages(
      [("human", "{{msg.content}}")],
      template_format="jinja2"
  )
  # Raises SecurityError: Access to attributes is not allowed
  ```

  **Important Recommendation**: Due to the expressiveness of Jinja2 and the difficulty of fully sandboxing it, **we recommend reserving Jinja2 templates for trusted sources only**. If you need to accept template strings from untrusted users, use f-string or mustache templates with the new restrictions instead.

  While we've hardened the Jinja2 implementation, the nature of templating engines makes comprehensive sandboxing challenging. The safest approach is to only use Jinja2 templates when you control the template source.

  **Important Reminder**: Many applications do not need prompt templates. Templates are useful for variable substitution and dynamic logic (if statements, loops, conditionals). However, if you're building a chatbot or conversational application, you can often work directly with message objects (e.g., `HumanMessage`, `AIMessage`, `ToolMessage`) without templates. Direct message construction avoids template-related security concerns entirely.

  ## Remediation

  ### Immediate Actions

  1. **Audit your code** for any locations where template strings come from untrusted sources
  2. **Update to the patched version** of `langchain-core`
  3. **Review template usage** to ensure separation between template structure and user data

  ### Best Practices

  - **Consider if you need templates at all** - Many applications can work directly with message objects (`HumanMessage`, `AIMessage`, etc.) without templates
  - **Reserve Jinja2 for trusted sources** - Only use Jinja2 templates when you fully control the template content

  ## Update: Jinja2 Restrictions Reverted

  The Jinja2 hardening introduced in the initial patch has been **reverted as of `langchain-core` 1.1.3**. The restriction was not addressing a direct vulnerability but was part of broader defensive hardening. In practice, it significantly limited legitimate Jinja2 usage and broke existing templates. Since Jinja2 is intended to be used only with **trusted template sources**, the original behavior has been restored. Users should continue to avoid accepting untrusted template strings when using Jinja2, but no security issue exists with trusted templates.
impacted_packages:
  - purl: pkg:pypi/langchain-core
    affected_versions: vers:pypi/<=1.0.6
    fixed_versions: vers:pypi/1.0.7
    fixed_in_commits: []
    introduced_in_commits: []
  - purl: pkg:pypi/langchain-core
    affected_versions: vers:pypi/<=0.3.79
    fixed_versions: vers:pypi/0.3.80
    fixed_in_commits: []
    introduced_in_commits: []
severities:
  - score: '8.3'
    scoring_system: cvssv4
    scoring_elements: CVSS:4.0/AV:N/AC:L/AT:P/PR:N/UI:N/VC:H/VI:L/VA:N/SC:N/SI:N/SA:N
    published_at: None
    url: https://github.com/github/advisory-database/blob/main/advisories/github-reviewed/2025/11/GHSA-6qv9-48xg-fc7f/GHSA-6qv9-48xg-fc7f.json
  - score: HIGH
    scoring_system: generic_textual
    scoring_elements:
    published_at: None
    url:
weaknesses:
  - CWE-1336
references:
  - url: https://github.com/langchain-ai/langchain
    reference_type:
    reference_id:
  - url: https://github.com/langchain-ai/langchain/commit/c4b6ba254e1a49ed91f2e268e6484011c540542a
    reference_type:
    reference_id:
  - url: https://github.com/langchain-ai/langchain/commit/fa7789d6c21222b85211755d822ef698d3b34e00
    reference_type:
    reference_id:
  - url: https://github.com/langchain-ai/langchain/security/advisories/GHSA-6qv9-48xg-fc7f
    reference_type:
    reference_id:
  - url: https://nvd.nist.gov/vuln/detail/CVE-2025-65106
    reference_type:
    reference_id:
