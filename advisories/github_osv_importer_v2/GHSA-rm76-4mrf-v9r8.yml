advisory_id: GHSA-rm76-4mrf-v9r8
datasource_id: github_osv_importer_v2/GHSA-rm76-4mrf-v9r8
datasource_url: https://github.com/github/advisory-database/blob/main/advisories/github-reviewed/2025/02/GHSA-rm76-4mrf-v9r8/GHSA-rm76-4mrf-v9r8.json
aliases:
  - CVE-2025-25183
summary: "vLLM uses Python 3.12 built-in hash() which leads to predictable hash collisions in\
  \ prefix cache\n### Summary\n\nMaliciously constructed prompts can lead to hash collisions,\
  \ resulting in prefix cache reuse, which can interfere with subsequent responses and cause\
  \ unintended behavior.\n\n### Details\n\nvLLM's prefix caching makes use of Python's built-in\
  \ hash() function. As of Python 3.12, the behavior of hash(None) has changed to be a predictable\
  \ constant value. This makes it more feasible that someone could try exploit hash collisions.\n\
  \n### Impact\n\nThe impact of a collision would be using cache that was generated using different\
  \ content. Given knowledge of prompts in use and predictable hashing behavior, someone could\
  \ intentionally populate the cache using a prompt known to collide with another prompt in\
  \ use. \n\n### Solution\n\nWe address this problem by initializing hashes in vllm with a value\
  \ that is no longer constant and predictable. It will be different each time vllm runs. This\
  \ restores behavior we got in Python versions prior to 3.12.\n\nUsing a hashing algorithm\
  \ that is less prone to collision (like sha256, for example) would be the best way to avoid\
  \ the possibility of a collision. However, it would have an impact to both performance and\
  \ memory footprint. Hash collisions may still occur, though they are no longer straight forward\
  \ to predict.\n\nTo give an idea of the likelihood of a collision, for randomly generated\
  \ hash values (assuming the hash generation built into Python is uniformly distributed), with\
  \ a cache capacity of 50,000 messages and an average prompt length of 300, a collision will\
  \ occur on average once every 1 trillion requests.\n\n### References\n\n* https://github.com/vllm-project/vllm/pull/12621\n\
  * https://github.com/python/cpython/commit/432117cd1f59c76d97da2eaff55a7d758301dbc7\n* https://github.com/python/cpython/pull/99541"
impacted_packages:
  - purl: pkg:pypi/vllm
    affected_versions: vers:pypi/<0.7.2
    fixed_versions: vers:pypi/0.7.2
    fixed_in_commits: []
    introduced_in_commits: []
severities:
  - score: '2.6'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:H/PR:L/UI:R/S:U/C:N/I:L/A:N
    published_at: None
    url: https://github.com/github/advisory-database/blob/main/advisories/github-reviewed/2025/02/GHSA-rm76-4mrf-v9r8/GHSA-rm76-4mrf-v9r8.json
  - score: LOW
    scoring_system: generic_textual
    scoring_elements:
    published_at: None
    url:
weaknesses:
  - CWE-354
references:
  - url: https://github.com/pypa/advisory-database/tree/main/vulns/vllm/PYSEC-2025-62.yaml
    reference_type:
    reference_id:
  - url: https://github.com/python/cpython/commit/432117cd1f59c76d97da2eaff55a7d758301dbc7
    reference_type:
    reference_id:
  - url: https://github.com/python/cpython/pull/99541
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/commit/73b35cca7f3745d07d439c197768b25d88b6ab7f
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/pull/12621
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/security/advisories/GHSA-rm76-4mrf-v9r8
    reference_type:
    reference_id:
  - url: https://nvd.nist.gov/vuln/detail/CVE-2025-25183
    reference_type:
    reference_id: CVE-2025-25183
