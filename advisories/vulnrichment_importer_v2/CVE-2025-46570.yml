advisory_id: CVE-2025-46570
datasource_id: vulnrichment_importer_v2/CVE-2025-46570
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2025/46xxx/CVE-2025-46570.json
aliases: []
summary: vLLM is an inference and serving engine for large language models (LLMs). Prior to
  version 0.9.0, when a new prompt is processed, if the PageAttention mechanism finds a matching
  prefix chunk, the prefill process speeds up, which is reflected in the TTFT (Time to First
  Token). These timing differences caused by matching chunks are significant enough to be recognized
  and exploited. This issue has been patched in version 0.9.0.
impacted_packages: []
severities:
  - score: '2.6'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:H/PR:L/UI:R/S:U/C:L/I:N/A:N
    published_at: None
    url:
  - score: Track
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:N/A:N/T:P/P:M/B:A/M:M/D:T/2025-05-29T18:04:57Z/
    published_at: None
    url:
weaknesses:
  - CWE-208
references:
  - url: https://github.com/vllm-project/vllm/commit/77073c77bc2006eb80ea6d5128f076f5e6c6f54f
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/pull/17045
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/security/advisories/GHSA-4qjh-9fv9-r85r
    reference_type:
    reference_id:
