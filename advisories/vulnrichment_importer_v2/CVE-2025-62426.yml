advisory_id: CVE-2025-62426
datasource_id: vulnrichment_importer_v2/CVE-2025-62426
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2025/62xxx/CVE-2025-62426.json
aliases: []
summary: vLLM is an inference and serving engine for large language models (LLMs). From version
  0.5.5 to before 0.11.1, the /v1/chat/completions and /tokenize endpoints allow a chat_template_kwargs
  request parameter that is used in the code before it is properly validated against the chat
  template. With the right chat_template_kwargs parameters, it is possible to block processing
  of the API server for long periods of time, delaying all other requests. This issue has been
  patched in version 0.11.1.
impacted_packages: []
severities:
  - score: '6.5'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H
    published_at: None
    url:
  - score: Track
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:N/A:N/T:P/P:M/B:A/M:M/D:T/2025-11-24T17:12:00Z/
    published_at: None
    url:
weaknesses:
  - CWE-770
references:
  - url: https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/chat_utils.py#L1602-L1610
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/blob/2a6dc67eb520ddb9c4138d8b35ed6fe6226997fb/vllm/entrypoints/openai/serving_engine.py#L809-L814
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/commit/3ada34f9cb4d1af763fdfa3b481862a93eb6bd2b
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/pull/27205
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/security/advisories/GHSA-69j4-grxj-j64p
    reference_type:
    reference_id:
