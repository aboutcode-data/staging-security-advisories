advisory_id: CVE-2026-21869
datasource_id: vulnrichment_importer_v2/CVE-2026-21869
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2026/21xxx/CVE-2026-21869.json
aliases: []
summary: llama.cpp is an inference of several LLM models in C/C++. In commits 55d4206c8 and
  prior, the n_discard parameter is parsed directly from JSON input in the llama.cpp server's
  completion endpoints without validation to ensure it's non-negative. When a negative value
  is supplied and the context fills up, llama_memory_seq_rm/add receives a reversed range and
  negative offset, causing out-of-bounds memory writes in the token evaluation loop. This deterministic
  memory corruption can crash the process or enable remote code execution (RCE). There is no
  fix at the time of publication.
impacted_packages: []
severities:
  - score: '8.8'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:L/PR:N/UI:R/S:U/C:H/I:H/A:H
    published_at: None
    url:
  - score: Track*
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:P/A:N/T:T/P:M/B:A/M:M/D:R/2026-01-08T19:15:25Z/
    published_at: None
    url:
weaknesses:
  - CWE-787
references:
  - url: https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-8947-pfff-2f3c
    reference_type:
    reference_id:
