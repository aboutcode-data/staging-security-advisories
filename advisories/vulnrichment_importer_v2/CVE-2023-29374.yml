advisory_id: CVE-2023-29374
datasource_id: vulnrichment_importer_v2/CVE-2023-29374
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2023/29xxx/CVE-2023-29374.json
aliases: []
summary: In LangChain through 0.0.131, the LLMMathChain chain allows prompt injection attacks
  that can execute arbitrary code via the Python exec method.
impacted_packages: []
severities:
  - score: '9.8'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:H/A:H
    published_at: None
    url:
  - score: Track
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:N/A:Y/T:T/P:M/B:A/M:M/D:T/2025-02-12T16:14:23Z/
    published_at: None
    url:
weaknesses: []
references:
  - url: https://github.com/hwchase17/langchain/issues/1026
    reference_type:
    reference_id:
  - url: https://github.com/hwchase17/langchain/issues/814
    reference_type:
    reference_id:
  - url: https://github.com/hwchase17/langchain/pull/1119
    reference_type:
    reference_id:
  - url: https://twitter.com/rharang/status/1641899743608463365/photo/1
    reference_type:
    reference_id:
