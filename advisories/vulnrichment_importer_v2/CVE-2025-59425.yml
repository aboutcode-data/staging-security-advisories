advisory_id: CVE-2025-59425
datasource_id: vulnrichment_importer_v2/CVE-2025-59425
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2025/59xxx/CVE-2025-59425.json
aliases: []
summary: vLLM is an inference and serving engine for large language models (LLMs). Before version
  0.11.0rc2, the API key support in vLLM performs validation using a method that was vulnerable
  to a timing attack. API key validation uses a string comparison that takes longer the more
  characters the provided API key gets correct. Data analysis across many attempts could allow
  an attacker to determine when it finds the next correct character in the key sequence. Deployments
  relying on vLLM's built-in API key validation are vulnerable to authentication bypass using
  this technique. Version 0.11.0rc2 fixes the issue.
impacted_packages: []
severities:
  - score: '7.5'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:L/PR:N/UI:N/S:U/C:H/I:N/A:N
    published_at: None
    url:
  - score: Track
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:N/A:Y/T:P/P:M/B:A/M:M/D:T/2025-10-07T14:32:10Z/
    published_at: None
    url:
weaknesses:
  - CWE-385
references:
  - url: https://github.com/vllm-project/vllm/blob/4b946d693e0af15740e9ca9c0e059d5f333b1083/vllm/entrypoints/openai/api_server.py#L1270-L1274
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/commit/ee10d7e6ff5875386c7f136ce8b5f525c8fcef48
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/releases/tag/v0.11.0
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/security/advisories/GHSA-wr9h-g72x-mwhm
    reference_type:
    reference_id:
