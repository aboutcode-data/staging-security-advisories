advisory_id: CVE-2025-53630
datasource_id: vulnrichment_importer_v2/CVE-2025-53630
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2025/53xxx/CVE-2025-53630.json
aliases: []
summary: llama.cpp is an inference of several LLM models in C/C++. Integer Overflow in the gguf_init_from_file_impl
  function in ggml/src/gguf.cpp can lead to Heap Out-of-Bounds Read/Write. This vulnerability
  is fixed in commit 26a48ad699d50b6268900062661bd22f3e792579.
impacted_packages: []
severities:
  - score: '8.9'
    scoring_system: cvssv4
    scoring_elements: CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N/E:P
    published_at: None
    url:
  - score: Track
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:P/A:N/T:P/P:M/B:A/M:M/D:T/2025-07-10T20:30:57Z/
    published_at: None
    url:
weaknesses:
  - CWE-680
  - CWE-122
references:
  - url: https://github.com/ggml-org/llama.cpp/commit/26a48ad699d50b6268900062661bd22f3e792579
    reference_type:
    reference_id:
  - url: https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-vgg9-87g3-85w8
    reference_type:
    reference_id:
