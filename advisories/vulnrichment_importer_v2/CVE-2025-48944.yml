advisory_id: CVE-2025-48944
datasource_id: vulnrichment_importer_v2/CVE-2025-48944
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2025/48xxx/CVE-2025-48944.json
aliases: []
summary: vLLM is an inference and serving engine for large language models (LLMs). In version
  0.8.0 up to but excluding 0.9.0, the vLLM backend used with the /v1/chat/completions OpenAPI
  endpoint fails to validate unexpected or malformed input in the "pattern" and "type" fields
  when the tools functionality is invoked. These inputs are not validated before being compiled
  or parsed, causing a crash of the inference worker with a single request. The worker will
  remain down until it is restarted. Version 0.9.0 fixes the issue.
impacted_packages: []
severities:
  - score: '6.5'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:N/I:N/A:H
    published_at: None
    url:
  - score: Track
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:P/A:N/T:P/P:M/B:A/M:M/D:T/2025-05-30T18:56:49Z/
    published_at: None
    url:
weaknesses:
  - CWE-20
references:
  - url: https://github.com/vllm-project/vllm/pull/17623
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/security/advisories/GHSA-vrq3-r879-7m65
    reference_type:
    reference_id:
