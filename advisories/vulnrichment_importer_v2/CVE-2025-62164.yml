advisory_id: CVE-2025-62164
datasource_id: vulnrichment_importer_v2/CVE-2025-62164
datasource_url: https://github.com/cisagov/vulnrichment/blob/develop/2025/62xxx/CVE-2025-62164.json
aliases: []
summary: vLLM is an inference and serving engine for large language models (LLMs). From versions
  0.10.2 to before 0.11.1, a memory corruption vulnerability could lead to a crash (denial-of-service)
  and potentially remote code execution (RCE), exists in the Completions API endpoint. When
  processing user-supplied prompt embeddings, the endpoint loads serialized tensors using torch.load()
  without sufficient validation. Due to a change introduced in PyTorch 2.8.0, sparse tensor
  integrity checks are disabled by default. As a result, maliciously crafted tensors can bypass
  internal bounds checks and trigger an out-of-bounds memory write during the call to to_dense().
  This memory corruption can crash vLLM and potentially lead to code execution on the server
  hosting vLLM. This issue has been patched in version 0.11.1.
impacted_packages: []
severities:
  - score: '8.8'
    scoring_system: cvssv3.1
    scoring_elements: CVSS:3.1/AV:N/AC:L/PR:L/UI:N/S:U/C:H/I:H/A:H
    published_at: None
    url:
  - score: Track
    scoring_system: ssvc
    scoring_elements: SSVCv2/E:N/A:N/T:T/P:M/B:A/M:M/D:T/2025-11-24T17:15:13Z/
    published_at: None
    url:
weaknesses:
  - CWE-787
  - CWE-20
  - CWE-502
  - CWE-123
references:
  - url: https://github.com/vllm-project/vllm/commit/58fab50d82838d5014f4a14d991fdb9352c9c84b
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/pull/27204
    reference_type:
    reference_id:
  - url: https://github.com/vllm-project/vllm/security/advisories/GHSA-mrw7-hf4f-83pf
    reference_type:
    reference_id:
