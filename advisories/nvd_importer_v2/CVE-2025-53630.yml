advisory_id: CVE-2025-53630
datasource_id: nvd_importer_v2/CVE-2025-53630
datasource_url: https://nvd.nist.gov/vuln/detail/CVE-2025-53630
aliases: []
summary: llama.cpp is an inference of several LLM models in C/C++. Integer Overflow in the gguf_init_from_file_impl
  function in ggml/src/gguf.cpp can lead to Heap Out-of-Bounds Read/Write. This vulnerability
  is fixed in commit 26a48ad699d50b6268900062661bd22f3e792579.
impacted_packages: []
severities:
  - score: '8.9'
    scoring_system: cvssv4
    scoring_elements: CVSS:4.0/AV:N/AC:L/AT:N/PR:N/UI:N/VC:H/VI:H/VA:H/SC:N/SI:N/SA:N/E:P/CR:X/IR:X/AR:X/MAV:X/MAC:X/MAT:X/MPR:X/MUI:X/MVC:X/MVI:X/MVA:X/MSC:X/MSI:X/MSA:X/S:X/AU:X/R:X/V:X/RE:X/U:X
    published_at: None
    url: https://nvd.nist.gov/vuln/detail/CVE-2025-53630
weaknesses:
  - CWE-680
  - CWE-122
references:
  - url: https://github.com/ggml-org/llama.cpp/commit/26a48ad699d50b6268900062661bd22f3e792579
    reference_type:
    reference_id:
  - url: https://github.com/ggml-org/llama.cpp/security/advisories/GHSA-vgg9-87g3-85w8
    reference_type:
    reference_id:
  - url: https://nvd.nist.gov/vuln/detail/CVE-2025-53630
    reference_type:
    reference_id: CVE-2025-53630
